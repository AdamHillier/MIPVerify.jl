{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how you can find adversarial examples for a pre-trained example network on the MNIST dataset.\n",
    "\n",
    "We suggest having the `Gurobi` solver installed, since its performance is significantly faster. If this is not possible, the `Cbc` solver is another option.\n",
    "\n",
    "The `Images` package is only necessary for visualizing the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using MIPVerify\n",
    "using Gurobi\n",
    "using Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "## MNIST dataset\n",
    "\n",
    "We begin by loading the MNIST dataset. The data is provided as a Julia `struct` for easy access. The training images and test images are provided as a 4-dimensional array of size `(num_samples, height, width, num_channels)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST:\n",
      "  `train`: {ImageDataset}\n",
      "    `images`: 55000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
      "    `labels`: 55000 corresponding labels, with 10 unique labels in [0, 9].\n",
      "  `test`: {ImageDataset}\n",
      "    `images`: 10000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
      "    `labels`: 10000 corresponding labels, with 10 unique labels in [0, 9]."
     ]
    }
   ],
   "source": [
    "mnist = MIPVerify.read_datasets(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ImageDataset}\n",
      "    `images`: 55000 images of size (28, 28, 1), with pixels in [0.0, 1.0].\n",
      "    `labels`: 55000 corresponding labels, with 10 unique labels in [0, 9]."
     ]
    }
   ],
   "source": [
    "mnist.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `Images.colorview` to preview these images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAP+Hj8y/AAAAB3RJTUUH4QwRBh06AXR2YAAAAWxJREFUaN7t2c8rBGEcx/E9cHRR0p6UotyUk8M6cZKLpDhxdZNw2YMz8h+4ipu7uHKSlByIQinKz0hk8/7WTk0TGuuZeR76vI67z857p31qvzNTKIiIiIiIiORsHGMYwAxKUDCs4Cjmcf2Ft6oXVPAEe30LTVDQb3ARdrBKjTbRDAX9Bc9gH9zDdsICSp+Ywwni0dSbR0HnwXYMogGpf3i04gBRdAoK+gn+xhCi4BUUVFDBfIMTWEYUfEQXFAwzWMQ0LmLekRyo7qBgWMFezOIYaYbiJSgYRrANG0hujFPsVvXDvtAhovdtYynoPziJI9gBHnAOG3JH0ILk+h3Y2nt0QEH/wTXYAdbRg+/WduIStr7mIUpB58FGlJFmbXwQXoGCYQR/wm4cWewG3VDwbwX38QoLriLzs1PQOftzttgtctmhCjplA5U9XLHBaRiZn52CTtXDLmaeYTeGMj87BZ2rg1149iHzmILyv30AhRSJ2miNU48AAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{Float32},2}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorview(Gray, mnist.train.images[1, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55000-element Array{Int64,1}:\n",
       " 7\n",
       " 3\n",
       " 4\n",
       " 6\n",
       " 1\n",
       " 8\n",
       " 1\n",
       " 0\n",
       " 9\n",
       " 8\n",
       " 0\n",
       " 3\n",
       " 1\n",
       " ⋮\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 2\n",
       " 9\n",
       " 5\n",
       " 1\n",
       " 8\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Neural Network\n",
    "\n",
    "We import a sample pre-trained neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convolutional neural net MNIST.n1\n",
      "  `convlayer_params` [0]:\n",
      "    (none)\n",
      "  `fclayer_params` [2]:\n",
      "    fully connected layer with 784 inputs and 40 output units, and a ReLU activation function.\n",
      "    fully connected layer with 40 inputs and 20 output units, and a ReLU activation function.\n",
      "  `softmax_params`:\n",
      "    softmax layer with 20 inputs and 10 output units."
     ]
    }
   ],
   "source": [
    "n1params = MIPVerify.get_example_network_params(\"MNIST.n1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MIPVerify.num_correct` allows us to verify that the network has a reasonable accuracy on the test set of 96.95%. (This step is crucial when working with your own neural net parameters; since the training is done outside of Julia, a common mistake is to transfer the parameters incorrectly.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9695"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIPVerify.num_correct(n1params, \"MNIST\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We feed the first image into the neural net, obtaining the activations of the final softmax layer. \n",
    "\n",
    "Note that the image must be specified as a 4-dimensional array with size `(1, height, width, num_channels)`. We provide a helper function `MIPVerify.get_image` that extracts the image from the dataset while preserving all four dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×28×28×1 Array{Float32,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 26, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 27, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 28, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_image = MIPVerify.get_image(mnist.train.images, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float32,1}:\n",
       " 0.15597 \n",
       " 0.270125\n",
       " 1.49147 \n",
       " 0.145112\n",
       " 0.281066\n",
       " 0.385918\n",
       " 0.228231\n",
       " 4.10202 \n",
       " 0.905381\n",
       " 1.15467 "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_activations = sample_image |> n1params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The category that has the largest activation is category 8, corresponding to a label of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(output_activations |> MIPVerify.get_max_index) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches the true label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MIPVerify.get_label(mnist.train.labels, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding an Adversarial Example\n",
    "\n",
    "We now try to find an adversarial example for the first image on `n1params`, setting the target category as index `9` (corresponding to a true label of 8).\n",
    "\n",
    "Named arguments are described below.\n",
    "  + `pp`: Specifies the family of perturbations allowed. \n",
    "    + Defaults to `MIPVerify.AdditivePerturbationParameters()`.\n",
    "    + Options are `MIPVerify.AdditivePerturbationParameters()`, corresponding to _additive perturbations_ where each pixel can be modified independently, and `MIPVerify.BlurPerturbationParameters((blur_height, blur_width))`, corresponding to _blurring perturbations_ simulating camera shake.\n",
    "  + `norm_order`: Specifies the order of the perturbation norm that we are attempting to minimize.\n",
    "    + Defaults to 1. \n",
    "    + Options are `1, 2, Inf`.\n",
    "  + `tolerance`: Specifies the amount by which the activation in the softmax layer of the target category needs to exceed the activation in the softmax layer of other categories. \n",
    "    + Defaults to 0. \n",
    "    + Larger values of `tolerance` correspond to the neural net being more certain that the perturbed image has the target label.\n",
    "  + `rebuild`: Specifies whether to ignore cached models and rebuild the model from scratch. \n",
    "    + Defaults to `true`.\n",
    "\n",
    "(You can safely ignore the messages that look like `WARNING: Not solved to optimality, status: UserLimit`. These are generated as part of the model-building process, and will be muted in a future release)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimize a model with 3385 rows, 3256 columns and 71132 nonzeros\n",
      "Variable types: 3196 continuous, 60 integer (60 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [2e-05, 7e+02]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+02]\n",
      "  RHS range        [1e-02, 7e+02]\n",
      "Presolve removed 2860 rows and 2184 columns\n",
      "Presolve time: 0.13s\n",
      "Presolved: 525 rows, 1072 columns, 65472 nonzeros\n",
      "\n",
      "MIP start did not produce a new incumbent solution\n",
      "MIP start violates constraint R1024 by 1.000000000\n",
      "\n",
      "Variable types: 1012 continuous, 60 integer (60 binary)\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 252 iterations, 0.01 seconds\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    8          -    0.00000      -     -    0s\n",
      "Another try with MIP start\n",
      "H    0     0                      55.3332559    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    9   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0   11   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    7   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   55.33326    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    6   55.33326    0.00000   100%     -    0s\n",
      "H    0     0                      25.7943430    0.00000   100%     -    0s\n",
      "     0     2    0.00000    0    1   25.79434    0.00000   100%     -    0s\n",
      "H   31    28                      10.0907739    0.00000   100%  73.7    0s\n",
      "*  100    70              28      10.0355213    0.00000   100%  64.2    1s\n",
      "  3854   393    8.87927   25    5   10.03552    5.15482  48.6%  58.4    5s\n",
      "* 5206   322              27       9.7569972    6.82950  30.0%  55.8    6s\n",
      "\n",
      "Cutting planes:\n",
      "  Gomory: 2\n",
      "  Cover: 3\n",
      "  MIR: 9\n",
      "  Flow cover: 11\n",
      "\n",
      "Explored 7706 nodes (423206 simplex iterations) in 8.16 seconds\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 5: 9.757 10.0355 10.0908 ... 55.3333\n",
      "Pool objective bound 9.757\n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 9.756997241475e+00, best bound 9.756997241475e+00, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Symbol,Any} with 7 entries:\n",
       "  :PerturbationParameters => MIPVerify.AdditivePerturbationParameters()\n",
       "  :SolveStatus            => :Optimal\n",
       "  :Output                 => JuMP.GenericAffExpr{Float64,JuMP.Variable}[-0.0120…\n",
       "  :Input                  => JuMP.Variable[__anon__ __anon__ __anon__ __anon__ …\n",
       "  :Model                  => Minimization problem with:…\n",
       "  :Perturbation           => JuMP.Variable[__anon__ __anon__ __anon__ __anon__ …\n",
       "  :PerturbedInput         => JuMP.Variable[__anon__ __anon__ __anon__ __anon__ …"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_label_index = 9\n",
    "d = MIPVerify.find_adversarial_example(n1params, sample_image, target_label_index, GurobiSolver, rebuild=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×28×28×1 Array{Float64,4}:\n",
       "[:, :, 1, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 2, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 3, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 26, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 27, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[:, :, 28, 1] =\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JuMP\n",
    "\n",
    "perturbed_sample_image = getvalue(d[:PerturbedInput])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we feed the perturbed image into the neural net and inspect the activation in the final layer. We verify that the perturbed image does maximize the activation of the target label index, which is 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       " 0.257004\n",
       " 0.416757\n",
       " 0.692725\n",
       " 0.38037 \n",
       " 0.295063\n",
       " 0.204749\n",
       " 0.488696\n",
       " 3.30817 \n",
       " 3.30817 \n",
       " 0.551406"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbed_sample_image |> n1params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the perturbed image and compare it to the original image. Since we are minimizing the L1-norm, changes are made to only a few pixels, but the magnitude of these changes are large (and noticeable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAP+Hj8y/AAAAB3RJTUUH4QwRBhUZa8qNGgAAAZhJREFUaN7t2c8rBGEcx/E9cHRRkpNSlJtyclgnTnKRFCeubhIuDs7IH6Bcxc1dXDlJCgfiYEtRfkYiy/tbOzWNnWnYZ3Zm9Xldtnaeed477dR+dzeXEwlYhYK1HRQR976gYG0HRSRbJjCOQcwiDwWzFRzDIm5DfJS8oYgX2PO7aIKC6QaXYZsV/2gHzVAwveAJ7MRD7AUsIV/GAi7gj8a+eRR0HuzAEBoQ+41HG47hRadRbt2PgVpB58FKDMML3kDBbAb9JwRPjgp6axVMPxj2AiaxBi/4jG5UtLmCiQVbMIMCLGiPnwgOVA9QMFvBPszhHLahxaKG4hUomI1gO7YRvDEucVAyAHtBp/CO242lYPrBKZzBNnjCFWzIHUUrguv3YWsf0QkF0w9uwjbYQi+i1nbhGrY+bBBWsPrBRswjzlr/ILwO/7HYA7CCzoO/YT8cWewOPXAeUDDR2BHeYcENJH51CjpnH84Wu0dV7lAFnbKByv5cscFpBIlfnYJO1cO+zLzCfhhK/OoUdK4O9sWzH4nHFJT/7RuBJiP5Y/StAQAAAABJRU5ErkJggg==",
      "text/plain": [
       "28×28 Array{Gray{Float64},2}:\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)  …  Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)\n",
       " Gray{Float64}(0.0)  Gray{Float64}(0.0)     Gray{Float64}(0.0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colorview(Gray, perturbed_sample_image[1, :, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAAABGdBTUEAALGPC/xhBQAAAAJiS0dEAP+Hj8y/AAAAB3RJTUUH4QwRBhkt5ss2owAAAWxJREFUaN7t2c8rBGEcx/E9cHRR0p6UotyUk8M6cZKLpDhxdZNw2YMz8h+4ipu7uHKSlByIQinKz0hk8/7WTk0TGuuZeR76vI67z857p31qvzNTKIiIiIiIiORsHGMYwAxKUDCs4Cjmcf2Ft6oXVPAEe30LTVDQb3ARdrBKjTbRDAX9Bc9gH9zDdsICSp+Ywwni0dSbR0HnwXYMogGpf3i04gBRdAoK+gn+xhCi4BUUVFDBfIMTWEYUfEQXFAwzWMQ0LmLekRyo7qBgWMFezOIYaYbiJSgYRrANG0hujFPsVvXDvtAhovdtYynoPziJI9gBHnAOG3JH0ILk+h3Y2nt0QEH/wTXYAdbRg+/WduIStr7mIUpB58FGlJFmbXwQXoGCYQR/wm4cWewG3VDwbwX38QoLriLzs1PQOftzttgtctmhCjplA5U9XLHBaRiZn52CTtXDLmaeYTeGMj87BZ2rg1149iHzmILyv30AhRSJ2miNU48AAAAASUVORK5CYII=",
      "text/plain": [
       "28×28 Array{Gray{Float64},2}:\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)  …  Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)  …  Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)  …  Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " ⋮                                         ⋱                     \n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)  …  Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)  …  Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)\n",
       " Gray{Float64}(-0.0)  Gray{Float64}(-0.0)     Gray{Float64}(-0.0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_image = getvalue(d[:Input])\n",
    "colorview(Gray, original_image[1, :, :, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That concludes this quickstart! The next tutorial will introduce you to each of the layers, and show how you can import your own neural network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
